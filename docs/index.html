<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <title> Regression Model Based on Best Response Correlated Convex Combinations </title>

        <link rel="stylesheet" href="http://lab.hakim.se/reveal-js/css/reveal.css">
        <link rel="stylesheet" href="http://lab.hakim.se/reveal-js/css/theme/black.css">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="http://lab.hakim.se/reveal-js/lib/css/zenburn.css">

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'http://lab.hakim.se/reveal-js/css/print/pdf.css' : 'http://lab.hakim.se/reveal-js/css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>
    </head>
    <body>
        <!-- Slides are here -->
        <div class="reveal">
            <div class="slides">
                <section>
                    <h2> Regression Model Based on Best Response Correlated Convex Combinations </h2>
                    <h3> Khomutov Nikita </h3> 
                    <small>
                        <p> CMC MSU </p>
                        <p> Moscow, 2016 </p>
                    </small>
                </section>

                <!-- Ensembles -->
                <section>
                    <h2> Algorithm Ensembles </h2>
                    <p>The usage of different forecasting/recognition methods and training on different training sets and freature subsets allow us to build some forecasting/recognition algorithms $A_1, \dots, A_r$. </p>
                    <p class="fragment fade-up"> We can try to increase generalization power by selecting an algorithm with minimal error.
                    However, the most effective result often can be acheived by using all the algorithms $A_1, \dots, A_r$ </p>
                    <p class="fragment fade-up"> The usage of an ensemble of algorithms which are based on different models allow us to take advantage of their different extrapolation principles </p>
                </section>

                <!-- Convex -->
                <section>
                    <section>
                        <h2> Convex combinations </h2>
                        <p>The usage of an ensemble can be statistically rationalized by analysing convex combination error</p>
                        <p> Let suppose that algorithms $A_1, \dots, A_r$ forecast the value of variable $Y$ </p>
                        <p class="fragment fade-up"> Let $f_i$ is a forecast computed by $A_i$. Then
                        $$ \Delta_i = E_\Omega (Y-f_i)^2 $$
                        is an expectation of square error for $A_i$ </p>
                        <p class="fragment fade-up"> Let $\rho_{ij}$ be expectation of square deviation for forecasts of $A_i$ and $A_j$. We'll call it later as <q> "distance" </q>
                        $$ \rho_{ij} = E_\Omega (f_i - f_j)^2 $$ </p>
                    </section>

                    <section>
                        <p> Let $c_1, \dots, c_r$ be positive coefficients that sums to $1$. Denote convex combitaion of forecasts of $A_1, \dots, A_r$ as $\hat{f}$, i.e.
                        $$ \hat{f} = \sum_{i=1}^{r} c_i f_i \qquad \sum_{i=1}^r c_i = 1 $$ </p>
                        <p class="fragment fade-up"> It can be shown that the error of a convex combination is
                        $$ \hat{\Delta} = E_\Omega(Y-\hat{f})^2 = \sum_{i=1}^r c_i \Delta_i - \frac12 \sum_{i=1}^r \sum_{j=1}^r c_i c_j \rho_{ij} $$ </p>
                    </section>

                    <section>
                        <p> $$ \hat{\Delta} = E_\Omega(Y-\hat{f})^2 = \sum_{i=1}^r c_i \Delta_i - \frac12 \sum_{i=1}^r \sum_{j=1}^r c_i c_j \rho_{ij} $$ </p>
                        <p class="fragment fade-up"> Pay attention to $\rho_{ij}$ non-negativity and $c_i$ posivity:
                        $$ \hat{\Delta} \leq \sum_{i=1}^r c_i \Delta_i $$ </p>
                        <p class="fragment fade-up"> The expectation of convex combination square error always doesn't exceed corresponding convex combination of individual square errors </p>
                        <p class="fragment fade-up"> Additionally, the higher distance $\rho_{ij}$ increases the difference in inequality. That's why algorithms based on different principles are effective together </p>
                    </section>
                </section>

                <!-- Linreg -->
                <section>
                    <section>                    
                        <h2> Linear Regression </h2>
                        <p class="fragment fade-up"> Consider conventional problem of multidimensional regressional analysis. The variable $Y$ is predicted on variables $X_1, \dots, X_n$ by linear regression 
                        $$ \beta_0 + \sum_{i=1}^n \beta_i X_i $$ </p>
                        <p class="fragment fade-up"> Popular methods solve the problem
                        $$ \min_{\beta \in \mathbb{R}^{n+1}} \left\{ \sum_{j=1}^n \left( y_j - \beta_0 - \sum_{i=1}^n \beta_i x_{ij} \right)^2 + P(\beta) \right\} $$ 
                        with regularizing penalty term $P(\beta)$
                        </p>
                    </section>
                    <section>
                        <table>
                            <thead>
                                <tr><th>Method</th> <th>Penalty</th> </tr>
                            </thead>
                            <tbody>
                                <tr class="fragment fade-up">
                                    <th>Lasso</th> <th>$\sum_{i=1}^n |\beta_i|$</th>
                                </tr>
                                <tr class="fragment fade-up">
                                    <th>Ridge regression</th> <th>$\sum_{i=1}^n \beta_i^2$</th>
                                </tr>
                                <tr class="fragment fade-up">
                                    <th>ElasticNet</th> <th> $(1-\alpha) \sum_{i=1}^n \beta_i^2 + \alpha\sum_{i=1}^n |\beta_i|$</th>
                                </tr>
                            </tbody>
                        </table>
                        <p class="fragment fade-up"> Consider another way to introduce regularization
                        \begin{eqnarray}
                            C_1(\beta_0, \dots, \beta_n) \geq 0 \\
                            \cdots \\
                            C_k(\beta_0, \dots, \beta_n) \geq 0 \nonumber
                        \end{eqnarray}
                        </p>
                        

                    </section>
                </section>

                <section>
                    TODO
                </section>

                <!-- Links -->
                <section>
                    <section>
                        <h2> References </h2>
                        <ul>
                            <li>Regression model based on convex combinations best correlated with response 
                            <small align="right"> Dokukin, A.A. & Senko, O.V. Comput. Math. and Math. Phys. (2015) 55: 526. doi:10.1134/S0965542515030045 </small>
                            </li>
                        </ul>                        
                    </section>
                    <section>
                        <p><h2>GitHub</h2></p>
                        <img src="http://i.imgur.com/7XPpKHS.png" style="background:none; box-shadow:none; image-rendering: pixelated;" width="40%" height="40%">
                        <p><a href="https://github.com/urandon/inmc3-py" style="color:gainsboro">urandon/inmc3-py</a></p>
                    </section>
                </section>

                <section>
                    <h1> Thank you for your attention </h1>
                </section>

            </div>
        </div>

        <!-- Imports -->
        <script src="http://lab.hakim.se/reveal-js/lib/js/head.min.js"></script>
        <script src="http://lab.hakim.se/reveal-js/js/reveal.js"></script>

        <script>
            // More info https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                history: true,

                    math: {
                        mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
                        config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                    },

                // More info https://github.com/hakimel/reveal.js#dependencies
                dependencies: [
                    //{ src: 'http://lab.hakim.se/reveal-js/socket.io/socket.io.js', async: true },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/notes-server/client.js', async: true },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/math/math.js', async: true },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/markdown/marked.js' },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/markdown/markdown.js' },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/notes/notes.js', async: true },
                    { src: 'http://lab.hakim.se/reveal-js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
                ]
            });
        </script>
    </body>
</html>
